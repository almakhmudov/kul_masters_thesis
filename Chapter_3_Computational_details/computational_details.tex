\chapter{Computational details}
This chapter provides detailed information on the computational methods employed in this work. The first section outlines the generation of the training dataset, including system preparation, initial equilibration using molecular mechanics, exploration of the configuration space at the GFN1-xTB level, further data labelling, and iterative training of the neural network potential. The second section discusses production runs at various temperatures using the fitted neural network potential. The third section describes the workflow for validating the transition states obtained from the simulations, based on the partial Hessian formalism. Finally, the fourth section presents the data analysis and visualisation techniques used to interpret the results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Training dataset generation}

\subsection{System preparation}
The systems were prepared using the functionality of the CHARMM-GUI webserver \citep{joCHARMMGUIWebbasedGraphical2008}, specifically the Multicomponent Assembler interface~\citep{kernCHARMMGUIMulticomponentAssembler2024}.

As a first step, the singly protonated and deprotonated forms of methyl diphosphate were parameterised using CGenFF~\citep{kimCHARMMGUILigandReader2017}, i.e., the CHARMM General Force Field. These protonation states were chosen based on the dissociation constants of pyrophosphoric (diphosphoric) acid~\citep{haynesCRCHandbookChemistry2016}:
\begin{align*}
    \mathrm{H_4P_2O_7} \rightleftharpoons \mathrm{[H_3P_2O_7]^-} + \mathrm{H^+},\quad \mathrm{p}K_\mathrm{a} = 0.91 \\
    \mathrm{[H_3P_2O_7]^-} \rightleftharpoons \mathrm{[H_2P_2O_7]^{2-}} + \mathrm{H^+},\quad \mathrm{p}K_\mathrm{a} = 2.10 \\
    \mathrm{[H_2P_2O_7]^{2-}} \rightleftharpoons \mathrm{[HP_2O_7]^{3-}} + \mathrm{H^+},\quad \mathrm{p}K_\mathrm{a} = 6.70 \\
    \mathrm{[HP_2O_7]^{3-}} \rightleftharpoons \mathrm{[P_2O_7]^{4-}} + \mathrm{H^+},\quad \mathrm{p}K_\mathrm{a} = 9.32
\end{align*}

Thus, at physiological pH (7.4), this acid exists in equilibrium between the singly and doubly deprotonated forms. Assuming the methyl group behaves similarly to a proton, the methyl diphosphate molecule was considered to exist as a mixture of the singly protonated (MeHDP) and deprotonated (MeDP) forms under physiological conditions.

Following successful parameterisation, the system was solvated in a cubic box of TIP3P water molecules, with sodium counterions (Na\textsuperscript{+}) added to neutralise the system's overall charge. The final system composition is provided in Table~\ref{tab:system-before-equilibration}.



\subsection{Initial equilibration using classical force fields}
The system equilibration followed the standard protocol generated by the CHARMM-GUI webserver~\citep{joCHARMMGUIWebbasedGraphical2008}. Initially, energy minimisation was conducted using the steepest descent algorithm for 5,000 steps.

This was followed by equilibration in the NVT (constant number of particles, volume, and temperature) ensemble for 5 ns. During both the minimisation and NVT phases, the solute's heavy atoms were restrained using a harmonic potential with a force constant of 400 kJ mol\textsuperscript{-1} nm\textsuperscript{-2}.

Subsequently, the system was equilibrated in the NPT (constant number of particles, pressure, and temperature) ensemble for 45 ns. Throughout this procedure, the temperature and pressure were maintained at 300 K and 1 bar, respectively. Temperature was controlled using a $\nu$-rescale thermostat~\citep{bussiCanonicalSamplingVelocity2007} with a coupling constant of 1 ps, and pressure was regulated using an isotropic \textit{c}-rescale barostat~\citep{bernettiPressureControlUsing2020} with a coupling constant of 5 ps. A 0.6 nm cut-off was applied for non-bonded interactions, and long-range electrostatics were treated using the Particle Mesh Ewald (PME) method. Periodic boundary conditions (PBC) were applied in all directions throughout the simulation.

\begin{table}[b]
    \centering
    \caption{System composition and simulation box details.}
    \label{tab:system-before-equilibration}
    \begin{tabular}{ccccc}
    \toprule
    \textbf{System} & \textbf{Final box dimensions (\AA\textsuperscript{3})} & \textbf{No. of H\textsubscript{2}O} & \textbf{No. of Na\textsuperscript{+}} & \textbf{No. of atoms} \\
    \midrule
    MeDP  & $15.877 \times 15.877 \times 15.877$ & 119 & 3 & 373 \\
    MeHDP & $15.901 \times 15.901 \times 15.901$ & 124 & 2 & 388 \\
    \bottomrule
    \end{tabular}
\end{table}

All simulations were carried out using GROMACS 2021.4~\citep{abrahamGROMACSHighPerformance2015} with CHARMM36m force field~\citep{huangCHARMM36mImprovedForce2017}. The leap-frog integrator was employed with a time step of 1 fs. All hydrogen-involving bonds were constrained using the LINCS algorithm. The equilibrated box dimensions used for subsequent simulations were taken from the output of the NPT run and are summarised in Table~\ref{tab:system-before-equilibration}. Unless otherwise stated, the last frame of the NPT simulations was used as the starting point for all further calculations.



\subsection{Collective variables}
To effectively sample the reaction space, two types of collective variables (CVs) were employed to bias the system: distances and coordination numbers (CNs). The coordination number is defined by the following smooth function:
\begin{equation}
    \sum_{i \in A} \sum_{j \in B} CN_{ij} = \frac{1 - \left( \frac{r_{ij} - d_0}{r_0} \right)^n}{1 - \left( \frac{r_{ij} - d_0}{r_0} \right)^m}
    \label{eq:coordination_number}
\end{equation}
where $r_{ij}$ is the distance between atoms $i$ and $j$ from groups $A$ and $B$, $d_0$ is the distance at which the CN begins to decay, $r_0$ is a characteristic decay length, and $n$ and $m$ are integers that control the steepness of the decay. Typically, $m > n$, ensuring a smooth transition of $CN_{ij}$ from approximately 1 to 0 as the distance increases.

The specific CVs used in this work are shown in Figure~\ref{fig:collective_variables}, and their corresponding parameters are as follows:

\begin{itemize}
    \item Distance between the $\beta$-phosphorus and the bridging oxygen (d\textsubscript{P-O}),
    \item Coordination number of all water oxygen atoms as well as the bridging oxygen surrounding the $\beta$-phosphorus (C\textsubscript{P-O}): $d_0 = 0$, $r_0 = 2.1$ \AA, $n = 8$, $m = 16$,
    \item Coordination number of non-methyl hydrogen atoms around 5 non-bridging and 1 bridging oxygen atoms (C\textsubscript{O-H}): $d_0 = 0$, $r_0 = 1.3$ \AA, $n = 8$, $m = 16$.
\end{itemize}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/3_Computational_details/methods_collective_variables.pdf}
    \caption{The definition of the collective variables (CVs) used in this work. CN stands for coordination number.}
    \label{fig:collective_variables}
\end{figure}

Additionally, the following CVs were monitored to check whether the system was in a reasonable region of the potential energy surface, e.g. to ensure that there is no oxygen exchange between the methyl diphosphate and the water molecules:
\begin{itemize}
    \item Coordination number of 3 nonbridging oxygens surrounding the $\beta$-phosphorus (C\textsubscript{P-O\textsubscript{nonbridging}}): $d_0 = 0$, $r_0 = 2.1$ \AA, $n = 8$, $m = 16$,
    \item Coordination number of water oxygens around the $\beta$-phosphorus (C\textsubscript{P-O\textsubscript{water}}): $d_0 = 0$, $r_0 = 2.1$ \AA, $n = 8$, $m = 16$,
\end{itemize}

To avoid sampling unphysical regions of the potential energy surface, quadratic (harmonic-like) wall potentials were applied to softly constrain certain degrees of freedom. The mathematical form of these wall potentials is given below:
\begin{equation}
    \text{For upper walls:} \; \sum_i k_i \left( \frac{CV_i - a_i + o_i}{s_i} \right)^{e_i}
    \label{eq:upper_wall}
\end{equation}
\begin{equation}
    \text{For lower walls:} \; \sum_i k_i \left| \frac{CV_i - a_i - o_i}{s_i} \right|^{e_i}
    \label{eq:lower_wall}
\end{equation}
Here, $CV_i$ denotes the value of the collective variable, $k_i$ is the force constant defining the wall's strength, $a_i$ is the central wall position, $o_i$ is an offset, $s_i$ is a scaling factor, and $e_i$ is the exponent that controls the wall’s steepness. When $e_i = 2$, the potential acts harmonically.

The wall potentials applied to the CVs during the simulations are summarised in Table~\ref{tab:wall_potentials}. The parameters for the wall potentials were chosen based on the expected ranges of the CVs. The force constants were set to ensure that the walls were sufficiently strong to prevent unphysical configurations while allowing for reasonable exploration of the configuration space.

\begin{table}[b!]
    \centering
    \caption{The restraints applied to the collective variables during some of the simulations. In all cases, $o = 0$, $s = 1$, $e = 2$. \textsuperscript{1}During the iterative training / production runs. \textsuperscript{2}Different values for the walls were used depending on the system MeDP/MeHDP. Distances are in \AA\ and coordination numbers are unitless.}
    \label{tab:wall_potentials}
    \begin{tabular}{cccc}
    \toprule
    \textbf{CV} & \textbf{Lower wall} & \textbf{Upper wall} & \textbf{Force constant (kcal mol\textsuperscript{-1} \AA\textsuperscript{-2})} \\
    \midrule
    d\textsubscript{P-O} & -- & 5.0 / 6.0$^{1}$ & 500  \\
    C\textsubscript{O-H} & -- / 1.3$^{2}$ & 1.3 / 2.5$^{2}$ & 1000 \\
    C\textsubscript{P-O\textsubscript{nonbridging}} & 2.6 & -- & 2000 \\
    C\textsubscript{P-O\textsubscript{water}} & -- & 1.3 & 2000 \\
    \bottomrule
    \end{tabular}
\end{table}

All CV-related computations were performed using the built-in tools of CP2K 2023.1 \citep{kuhneCP2KElectronicStructure2020} or PLUMED 2.9.3~\citep{tribelloPLUMED2New2014}. It is important to note that the number and type of CVs, as well as the applied restraints, varied depending on the specific stage of the workflow. In the following sections, the relevant collective variables and wall potentials will be specified accordingly.



\subsection{GFN1-xTB based exploration of the configuration space}
\label{subsec:xtb-exploration-of-configuration-space}
To generate the initial set of configurations for the training dataset, the system was subjected to molecular dynamics simulations using the semi-empirical GFN1-xTB~\citep{grimmeRobustAccurateTightBinding2017} level of theory. GFN1-xTB provides a good first approximation of the potential energy surface and is computationally efficient, thus making it suitable for relatively long MD simulations of large systems. 

Each system was first equilibrated for 5 ps in the NVT ensemble at 300 K to allow the structures to relax at the GFN1-xTB level, including a D3 dispersion correction~\citep{grimmeConsistentAccurateInitio2010}. Following equilibration, we performed 50 ps of well-tempered metadynamics (WTMD)~\citep{barducciWellTemperedMetadynamicsSmoothly2008} simulations in the NVT ensemble. In these simulations, a biasing potential was applied to encourage the system to explore regions of the configuration space beyond the reactant basin. This bias was introduced along two collective variables (CVs): the distance between the $\beta$-phosphorus and the oxygen atom connecting it to the rest of the molecule (d\textsubscript{P-O}), and the coordination number of all water oxygens together with the bridging oxygen surrounding the $\beta$-phosphorus atom (C\textsubscript{P-O}). No restraints were applied to the system during this stage.

All calculations were carried out using the CP2K 2023.1 package~\citep{kuhneCP2KElectronicStructure2020}. Temperature control was achieved using the $\nu$-rescale thermostat~\citep{bussiCanonicalSamplingVelocity2007}, with a time constant of 50 fs during equilibration and 100 fs during the WTMD simulations. The self-consistent field (SCF) convergence threshold was set to $10^{-5}$ a.u. The biasing potential was updated every 25 fs, with a Gaussian hill height of 2 kcal mol\textsuperscript{-1} and a width of 0.07 for each CV. The bias factor was set to 30. Finally, the integration time step was set to 0.5 fs. Throughout the simulations, periodic boundary conditions were applied in all directions.



\subsection{Data labeling}
All data points were labeled by performing single-point calculations to obtain the energy and force values. These single-point calculations were carried out using the Perdew–Burke–Ernzerhof (PBE) exchange-correlation functional~\citep{perdewGeneralizedGradientApproximation1996}, along with the D3 dispersion correction and the Becke-Johnson damping function~\citep{grimmeConsistentAccurateInitio2010,grimmeEffectDampingFunction2011}. In all calculations, the Goedecker-Teter-Hutter (GTH) pseudopotentials~\citep{goedeckerSeparableDualspaceGaussian1996,hartwigsenRelativisticSeparableDualspace1998} were used to represent the core electrons, in combination with the triple-$\zeta$ valence basis set with two polarisation functions (TZV2P).

The single-point calculations were performed using the Gaussian Plane Wave \; (GPW) method implemented in the QUICKSTEP module~\citep{vandevondeleQuickstepFastAccurate2005} of the CP2K 2023.1 package~\citep{kuhneCP2KElectronicStructure2020}. The SCF convergence threshold was set to 10\textsuperscript{-6} a.u. A plane-wave cutoff of 800 Ry was applied for the total density, while a cutoff of 60 Ry was used for the Kohn-Sham orbitals.

The aforementioned cutoffs were determined based on a convergence test performed on one of the configurations, as described in~\citep{cp2k_developersHowConvergeCUTOFF}. An error in total energy of less than 10\textsuperscript{-8} a.u. was considered acceptable for the convergence test. The test was conducted by varying the cutoff for the total density from 400 to 1500 Ry, and the cutoff for the Kohn-Sham orbitals from 10 to 200 Ry. The results of the convergence test are shown in Table~\ref{tab:cutoff-convergence-test}.



\subsection{Iterative training of the neural network potential} \label{subsec:iterative-training-of-the-neural-network-potential}
We trained a neural network potential using the NequIP framework~\citep{batznerE3equivariantGraphNeural2022}, which implements equivariant message-passing networks for atomistic simulations. Regarding the hyperparameters, a radial cutoff distance of 5.0 \AA\ was chosen to describe the atomic environment of the system.

The equivariant part of the neural network was composed of four interaction layers with a maximum tensor rank of $\ell = 1 \; \text{or} \; 2$. Feature parity was enabled to include both even and odd components, and 32 features per irreducible representation were used throughout. Scalar and gating nonlinearities were set to \texttt{silu} and \texttt{tanh} for even and odd parities, respectively. Eight radial basis functions were employed, in combination with a trainable Bessel basis and a polynomial cutoff of order 6.

The invariant subnetwork for radial interaction modelling consisted of two layers with 64 hidden neurons. Self-connections were enabled, and the average number of neighbours was computed automatically based on the dataset.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/3_Computational_details/methods_workflow_diagram.pdf}
    \caption{Iterative training of the NequIP neural network potential.}
    \label{fig:iterative-training}
\end{figure}

Training was performed using the Adam optimizer with the AMSGrad variant enabled, and with $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 10^{-8}$. A starting learning rate of 0.01 was used, and the learning rate was adaptively reduced by a factor of 0.5 upon stagnation of the validation loss (patience = 100 epochs). Early stopping was triggered if the validation loss remained unimproved for 50 epochs, if the loss dropped below $1 \times 10^{-5}$, or if it exceeded $1 \times 10^{4}$. The batch size was set to 5. The training was carried out over a period of three days on a single NVIDIA A100 GPU using float64 precision.

To thoroughly sample the reaction space, the training was performed in an iterative manner, where the model was first trained on a small set of data and then used to generate additional data points. This process was repeated until the model converged, with the RMSE of the atomic forces being less than 40 meV/\AA. The workflow is shown in Figure~\ref{fig:iterative-training}.

In the end, the full dataset consisted of 12,000 configurations for training and validation, and 1,200 configurations for testing, for both systems (MeDP and MeHDP) combined. This dataset was obtained within the three rounds of iterative training. In each round of training, the model was retrained on a larger dataset. The data obtained from each round will be discussed in the following sections.



\subsubsection{Selection of configurations for training and testing}

An important part of the iterative training process is the selection of configurations that will be used to train the neural network. To construct a representative and diverse dataset for training the neural network potential, configurations were selected from a metadynamics trajectory using a density-aware sampling strategy. The raw data were extracted from a file generated during the enhanced sampling simulations. Each configuration in this file corresponds to a simulation snapshot, annotated with a time index and two collective variables (CVs): the distance $d(\mathrm{O}_\text{remaining} - \mathrm{P}_\text{leaving})$ and the coordination number $\mathrm{CN}(\mathrm{P}_\text{leaving} - \mathrm{O}_\text{all})$.

The two CVs were combined into a two-dimensional feature space $\mathbf{X} = (d, \mathrm{CN})$, which served as the basis for sampling. This feature space often exhibits regions of highly non-uniform data density, due to the biased nature of metadynamics sampling. To account for this, a density-aware sampling method was employed to select configurations for training and testing that maintain good coverage across the feature space.

The selection procedure proceeds as follows:
\begin{enumerate}
    \item A user-defined number of samples is specified.
    \item K-means clustering is applied to the feature space to partition it into a number of clusters, $k$. The number of clusters is determined heuristically as $k = \max\left(10, \; \min\left(\left\lfloor \frac{N}{50} \right\rfloor, \left\lfloor \frac{n_\text{samples}}{10} \right\rfloor \right)\right)$, where $N$ is the total number of configurations and $n_\text{samples}$ is the desired number of samples.
    \item The number of points sampled from each cluster is proportional to its size, ensuring that denser regions do not dominate the dataset. A minimum of one sample is taken from each non-empty cluster.
    \item Within each cluster, a fixed number of configurations is randomly selected using a deterministic random seed to ensure reproducibility.
    \item After the training set is selected, the remaining configurations are used to construct the test set, following the same density-aware procedure while ensuring no overlap with the training configurations.
\end{enumerate}

This approach results in training and test datasets that closely mirror the overall distribution of the CVs, while ensuring that underrepresented regions of the feature space are adequately sampled. The final output consists of two lists of snapshot indices corresponding to the selected training and test configurations, along with their respective CV values. These snapshots were then extracted from the trajectory files for use in model training and evaluation. The pseudo-code for the density-aware sampling algorithm is provided in Algorithm~\ref{alg:density_aware_sampling}.



\subsubsection{First round}
In the first round of training the neural network potential, the model was trained on a small dataset consisting of 4,000 configurations. These configurations were obtained from the initial exploration of the configuration space at 300~K using the GFN1-xTB level of theory, as described in Section~\ref{subsec:xtb-exploration-of-configuration-space}. The enhanced sampling simulations were biased along d\textsubscript{P-O} and C\textsubscript{P-O}, and no restraints were applied to the system. The training was carried out using the hyperparameters described in Section~\ref{subsec:iterative-training-of-the-neural-network-potential}.



\subsubsection{Second round}
In the second round of training, the model was trained on a larger dataset consisting of 8,000 configurations. The additional configurations were obtained from a second round of exploration of the configuration space, driven by the neural network potential (NNP) obtained after the first round of training.

To run the simulations with the NNP, the LAMMPS package~\citep{thompsonLAMMPSFlexibleSimulation2022} compiled with PLUMED 2.9.3~\citep{tribelloPLUMED2New2014} and \texttt{pair\_nequip}~\citep{MirgroupPair_nequip} was used. The simulations were performed for 100~ps in the NVT ensemble at 300~K. The temperature was controlled by a Nos\'e--Hoover thermostat~\citep{noseUnifiedFormulationConstant1984, hooverCanonicalDynamicsEquilibrium1985} with a time constant of 50~fs. The biasing potential was applied to CV\textsubscript{1} and CV\textsubscript{2} every 50~fs, using a Gaussian hill height of 2~kcal~mol\textsuperscript{-1} and a width of 0.07 for each CV. The bias factor was set to 30, and the integration time step was 0.5~fs.

Restraints were applied to d\textsubscript{P-O} and C\textsubscript{P-O} in order to favour either a dissociative or associative mechanism of the reaction and sample more configurations from the transition state regions. The training was performed using the same hyperparameters as in the first round.



\subsubsection{Third round}
In the final round of training, the model was trained on a dataset consisting of 12,000 configurations. These additional configurations were obtained from a third round of exploration of the configuration space, driven by the NNP obtained after the second round of training. The simulations were performed for 500~ps using the same setup as in the second round. The only difference was that the temperature in this round was increased to 320~K and 340~K to explore the configuration space at higher temperatures. The same CVs were biased as in the previous run. No restraints were applied to the system. The training was conducted using the same hyperparameters as in the first round. The final dataset is summarised in Table~\ref{tab:full_dataset}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Production runs at different temperatures}
To thoroughly sample the reaction space, production runs were performed at various temperatures (300~K, 320~K, and 340~K) using the neural network potential obtained in the final round of training. To run the simulations with the NNP, the LAMMPS package~\citep{thompsonLAMMPSFlexibleSimulation2022}, compiled with PLUMED 2.9.3~\citep{tribelloPLUMED2New2014} and \texttt{pair\_nequip}~\citep{MirgroupPair_nequip}, was utilised. The simulations were carried out for 2~ns in the NVT ensemble, with temperature regulated by a Nos\'e--Hoover thermostat~\citep{noseUnifiedFormulationConstant1984, hooverCanonicalDynamicsEquilibrium1985} with a frequency of 50~fs\textsuperscript{-1}.

The biasing potential was applied every 50~fs to d\textsubscript{P-O}, C\textsubscript{P-O}, and C\textsubscript{O-H}, using a Gaussian hill height of 0.5~kcal~mol\textsuperscript{-1} and a width of 0.07 for each collective variable. Additionally, restraints were imposed on d\textsubscript{P-O}, C\textsubscript{O-H}, C\textsubscript{P-O\textsubscript{bridging}}, and C\textsubscript{P-O\textsubscript{water}}, as mentioned in Table~\ref{tab:wall_potentials}. The bias factor was set to 30, and the integration time step was maintained at 0.5~fs.

To obtain the free energy profiles of the reactions, the Gaussian kernels applied during the simulations were summed using the \texttt{sum\_hills} utility provided in the PLUMED 2.9.3 package~\citep{tribelloPLUMED2New2014}. After estimating the barrier heights, the corresponding rate constants (in s\textsuperscript{-1}) were calculated using the Eyring–Polanyi equation:
\begin{equation}
    k = \frac{\kappa k_B T}{h} e^{-\Delta G^\ddagger / RT}
\end{equation}
where $\kappa = 1$. Subsequently, the Arrhenius relationship was derived to obtain the activation energy barrier as the slope of the linear fit from the \(\log(k)\) versus \(1000/T\) plot.

With the rate constants determined, the corresponding half-lives were then calculated using the following equation:
\begin{equation}
    t_{1/2} = \frac{\ln(2)}{k}
    \label{eq:half_life}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Validation of the transition states}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lifetime of the transition states}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data analysis and visualisation}